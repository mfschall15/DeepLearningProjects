{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5_6oolQGtIXQ"
      },
      "outputs": [],
      "source": [
        "#dataloader.py\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "import PIL\n",
        "from tqdm import tqdm \n",
        "\n",
        "\n",
        "def rgb2int(arr):\n",
        "    \"\"\"\n",
        "    Convert (N,...M,3)-array of dtype uint8 to a (N,...,M)-array of dtype int32\n",
        "    \"\"\"\n",
        "    return arr[...,0]*(256**2)+arr[...,1]*256+arr[...,2]\n",
        "\n",
        "def rgb2vals(color, color2ind):\n",
        "   \n",
        "    int_colors = rgb2int(color)\n",
        "    int_keys = rgb2int(np.array(list(color2ind.keys()), dtype='uint8'))\n",
        "    int_array = np.r_[int_colors.ravel(), int_keys]\n",
        "    uniq, index = np.unique(int_array, return_inverse=True)\n",
        "    color_labels = index[:int_colors.size]\n",
        "    key_labels = index[-len(color2ind):]\n",
        "\n",
        "    colormap = np.empty_like(int_keys, dtype='int32')\n",
        "    colormap[key_labels] = list(color2ind.values())\n",
        "    out = colormap[color_labels].reshape(color.shape[:2])\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class TASDataset(Dataset):\n",
        "    def __init__(self, data_folder, eval=False, mode=None, augment_data=False):\n",
        "        self.data_folder = data_folder\n",
        "        self.eval = eval\n",
        "        self.mode = mode\n",
        "        self.augment_data = augment_data\n",
        "\n",
        "        # You can use any valid transformations here\n",
        "        # added augment_data flag, which is false by default, if true adds the following transformations.\n",
        "        if augment_data:\n",
        "            self.augment = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.RandomApply(transforms=transforms.RandomRotation(degrees=(0,30),fill=255), p=0.3),\n",
        "                # transforms.RandomApply(transforms=transforms.Compose([transforms.RandomCrop(size=(120,60)), transforms.Resize((768,384))]),p=0.3),                \n",
        "                # transforms.RandomRotation(degrees=(0,30),fill=255),\n",
        "                transforms.RandomHorizontalFlip(p=0.3),\n",
        "                transforms.RandomVerticalFlip(p=0.3),\n",
        "            ])\n",
        "            self.transform = transforms.Compose([transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "                                            ])\n",
        "        else:\n",
        "            self.transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "                                                ])                        \n",
        "\n",
        "        # we will use the following width and height to resize\n",
        "        self.width = 768\n",
        "        self.height = 384\n",
        "\n",
        "        self.color2class = {\n",
        "                #terrain\n",
        "                (192,192,192): 0, (105,105,105): 0, (160, 82, 45):0, (244,164, 96): 0, \\\n",
        "                #vegatation\n",
        "                ( 60,179,113): 1, (34,139, 34): 1, ( 154,205, 50): 1, ( 0,128,  0): 1, (0,100,  0):1, ( 0,250,154):1, (139, 69, 19): 1,\\\n",
        "                #construction\n",
        "                (1, 51, 73):2, ( 190,153,153): 2, ( 0,132,111): 2,\\\n",
        "                #vehicle\n",
        "                (0,  0,142):3, ( 0, 60,100):3, \\\n",
        "                #sky\n",
        "                (135,206,250):4,\\\n",
        "                #object\n",
        "                ( 128,  0,128): 5, (153,153,153):5, (255,255,  0 ):5, \\\n",
        "                #human\n",
        "                (220, 20, 60):6, \\\n",
        "                #animal\n",
        "                ( 255,182,193):7,\\\n",
        "                #void\n",
        "                (220,220,220):8, \\\n",
        "                #undefined\n",
        "                (0,  0,  0):9\n",
        "        }\n",
        "\n",
        "        self.input_folder = os.path.join(self.data_folder, 'train')\n",
        "        self.label_folder = os.path.join(self.data_folder, 'train_labels')\n",
        "\n",
        "        if self.eval:\n",
        "            self.input_folder = os.path.join(self.data_folder, 'val')\n",
        "            self.label_folder = os.path.join(self.data_folder, 'val_labels')\n",
        "        \n",
        "        image_names = os.listdir(self.input_folder)\n",
        "        \n",
        "        invalid_labels = ['1537962190852671077.png','1539600515553691119.png', '1539600738459369245.png','1539600829359771415.png','1567611260762673589.png']\n",
        "            \n",
        "        image_names = list(set(image_names).difference(set(invalid_labels)))\n",
        "            \n",
        "        self.paths = [(os.path.join(self.input_folder, i), os.path.join(self.label_folder, i)) for i in image_names]\n",
        "        \n",
        "        if self.mode == 'val': # use first 50 images for validation\n",
        "            self.paths = self.paths[:50]\n",
        "            \n",
        "        elif self.mode == 'test': # use last 50 images for test\n",
        "            self.paths = self.paths[50:]\n",
        "\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    \n",
        "    def __getitem__(self, idx):        \n",
        "            \n",
        "        image = np.asarray(PIL.Image.open(self.paths[idx][0]).resize((self.width, self.height)))\n",
        "        mask_image = np.asarray(PIL.Image.open(self.paths[idx][1]).resize((self.width, self.height), PIL.Image.NEAREST))\n",
        "        mask =  rgb2vals(mask_image, self.color2class)\n",
        "\n",
        "        # Since we are using flipping and rotations, we need to transform the binary mask as well.\n",
        "        if self.augment_data:\n",
        "            image = self.augment(image)\n",
        "            mask = self.augment(mask)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image).float()\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyEsDaFuWGrl",
        "outputId": "8c6b08e3-5fa2-42d3-d82c-5cf44d3a27f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fkcYkkT6tk_t"
      },
      "outputs": [],
      "source": [
        "#utils.py\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def get_loss_weights(weighting_method=None):\n",
        "# Get the loss according to different weighting methods, output will be tensor if not None\n",
        "  sample_per_class = np.load('sample_per_class.npy')\n",
        "  # sample_per_class = torch.from_numpy(sample_per_class.astype(np.float32))\n",
        "  # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  # sample_per_class = sample_per_class.to(device)\n",
        "  if weighting_method == 'INS':\n",
        "    loss_weights = ins_loss_weights(sample_per_class)\n",
        "  elif weighting_method == 'ISNS':\n",
        "    loss_weights = ins_loss_weights(sample_per_class, power=0.5)\n",
        "  elif weighting_method == 'ENS':\n",
        "    loss_weights = ens_loss_weights(sample_per_class)\n",
        "  elif weighting_method == 'basic':\n",
        "    loss_weights = basic_loss_weights(sample_per_class)\n",
        "  else:\n",
        "    return None\n",
        "  return loss_weights\n",
        "\n",
        "\n",
        "def ins_loss_weights(sample_per_class, power=1):\n",
        "  loss_weights = np.power(sample_per_class/sample_per_class.sum(), power)\n",
        "  loss_weights = 1/loss_weights\n",
        "  loss_weights = torch.from_numpy(loss_weights)\n",
        "  return loss_weights \n",
        "\n",
        "\n",
        "def basic_loss_weights(sample_per_class):\n",
        "  loss_weights = sample_per_class/sample_per_class.sum()\n",
        "  loss_weights = 1 - loss_weights\n",
        "  loss_weights = torch.from_numpy(loss_weights)\n",
        "  return loss_weights\n",
        "\n",
        "\n",
        "def ens_loss_weights(sample_per_class):\n",
        "  beta = 0.9\n",
        "  normalised_per_class = sample_per_class/sample_per_class.sum()\n",
        "  effective_num = 1 - np.power(beta,normalised_per_class)\n",
        "  loss_weights = effective_num / (1-beta)\n",
        "  loss_weights = 1/loss_weights\n",
        "  loss_weights = torch.from_numpy(loss_weights)\n",
        "  return loss_weights\n",
        "\n",
        "\n",
        "def iou(pred, target, n_classes = 10):\n",
        "  ious = []\n",
        "  pred = pred.view(-1)\n",
        "  target = target.view(-1)\n",
        "\n",
        "  # Ignore IoU for undefined class (\"9\")\n",
        "  for cls in range(n_classes-1):  # last class is ignored\n",
        "    pred_inds = pred == cls\n",
        "    target_inds = target == cls\n",
        "    \n",
        "    #BB - implemented intersection/union\n",
        "    intersection = (pred_inds*target_inds).sum().item()\n",
        "    union = pred_inds.sum().item() + target_inds.sum().item() - intersection\n",
        "    \n",
        "    if union == 0:\n",
        "      ious.append(float('nan'))  # If there is no ground truth, do not include in evaluation\n",
        "    else:\n",
        "      ious.append(intersection/union)\n",
        "\n",
        "  return np.array(ious)\n",
        "  \n",
        "\n",
        "def pixel_acc(pred, target, n_classes = 10):\n",
        "  \n",
        "  #BB - Keep track of total count\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  #Reshape\n",
        "  pred = pred.view(-1)\n",
        "  target = target.view(-1)\n",
        "\n",
        "  for cls in range(n_classes-1):\n",
        "\n",
        "    #BB - Identify preds and targets\n",
        "    pred_inds = pred == cls\n",
        "    target_inds = target == cls\n",
        "    \n",
        "    #BB - Add to correct if correct, and add to total samps\n",
        "    correct += (pred_inds*target_inds).sum().item()\n",
        "    total += target_inds.sum().item()\n",
        "  \n",
        "  #BB - Return fraction\n",
        "  return correct/total\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DWr9S6utZ3fN"
      },
      "outputs": [],
      "source": [
        "#starter.py\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import gc\n",
        "import copy\n",
        "\n",
        "\n",
        "#model save path:\n",
        "FILE = 'fcn_model.pth'\n",
        "\n",
        "#BB - Batch size?\n",
        "batchsize = 32\n",
        "\n",
        "path = '/content/drive/MyDrive/Colab_Notebooks/nn_hw3/tas500v1.1'\n",
        "train_dataset = TASDataset(path) \n",
        "val_dataset = TASDataset(path, eval=True, mode='val')\n",
        "test_dataset = TASDataset(path, eval=True, mode='test')\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batchsize, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batchsize, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batchsize, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WzubfxuU1qou"
      },
      "outputs": [],
      "source": [
        "#TESTING RESNET WITH TRANSFER LEARNING\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "   \n",
        "class FCN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_class):\n",
        "        super().__init__()\n",
        "        self.n_class = n_class\n",
        "        #pre-trained encoder\n",
        "        resnet = models.resnet34(pretrained=True)\n",
        "        self.features =  nn.Sequential(*(list(resnet.children())[:-2]))   #remove the fc layer and max pool layer\n",
        "        self.relu    = nn.ReLU(inplace=True)\n",
        "        #decode\n",
        "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn1     = nn.BatchNorm2d(512)\n",
        "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn2     = nn.BatchNorm2d(256)\n",
        "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn3     = nn.BatchNorm2d(128)\n",
        "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn4     = nn.BatchNorm2d(64)\n",
        "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn5     = nn.BatchNorm2d(32)\n",
        "        self.classifier = nn.Conv2d(32, self.n_class, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Encoder\n",
        "        x5 =  self.features(x)\n",
        "        # Decoder\n",
        "        y1 = self.bn1(self.relu(self.deconv1(x5)))\n",
        "        y2 = self.bn2(self.relu(self.deconv2(y1)))\n",
        "        y3 = self.bn3(self.relu(self.deconv3(y2)))\n",
        "        y4 = self.bn4(self.relu(self.deconv4(y3)))\n",
        "        y5 = self.bn5(self.relu(self.deconv5(y4)))    \n",
        "        score = self.classifier(y5)                   \n",
        "        # for param in self.features.parameters():\n",
        "        #   param.requires_grad = False\n",
        "\n",
        "        return score  # size=(N, n_class, x.H/1, x.W/1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb_sw5Na1q9h",
        "outputId": "7b43bc42-0f3e-49e8-d574-c46d0edff4ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class weights used\n"
          ]
        }
      ],
      "source": [
        "# def init_weights(m):\n",
        "#     if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "#         torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "#         torch.nn.init.normal_(m.bias.data) #xavier not applicable for biases   \n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "        torch.nn.init.normal_(m.bias.data) #xavier not applicable for biases   \n",
        "\n",
        "\n",
        "\n",
        "#BB - Changed epochs\n",
        "epochs = 100\n",
        "\n",
        "#N epochs patience\n",
        "epoch_no_imp = 0\n",
        "epoch_stop = 15\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "# Choose an appropriate loss function from https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html\n",
        "# The below function will return None by default, in order to get loss_weights, give weighting_method argument\n",
        "loss_weights = get_loss_weights(weighting_method='basic').type(torch.FloatTensor).to(device)\n",
        "# loss_weights = torch.FloatTensor(loss_weights).cuda()\n",
        "if loss_weights is not None:\n",
        "    criterion = nn.CrossEntropyLoss(weight=loss_weights) #BB - Chose CEL\n",
        "    print('class weights used')\n",
        "else:\n",
        "    criterion = nn.CrossEntropyLoss() #BB - Chose CEL\n",
        "n_class = 10\n",
        "fcn_model = FCN(n_class=n_class)\n",
        "fcn_model.apply(init_weights)\n",
        "\n",
        "#BB - Adding lr:\n",
        "lr = 0.001\n",
        "\n",
        "#batch size \n",
        "batchsize = 8\n",
        "\n",
        "#BB - added optimizer\n",
        "optimizer = optim.Adam(fcn_model.parameters(), lr=0.01) # choose an optimizer\n",
        "\n",
        "#BB - Adding lr scheduler:\n",
        "#scheduler = ExponentialLR(optimizer, 0.9) ###added schedule\n",
        "\n",
        "#BB - set device choice\n",
        "\n",
        "fcn_model = fcn_model.to(device) #transfer the model to the device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vguC6MYJ9XsY",
        "outputId": "88b18a43-c9ce-4d9e-a17c-616225073c51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(criterion.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6Q86Snor16nL"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "\n",
        "    best_iou_score = 0.0\n",
        "    best_acc = 0.0\n",
        "    best_loss = 0.0\n",
        "    epoch_no_imp = 0\n",
        "\n",
        "    train_loss = []\n",
        "    valid_loss= []\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        ts = time.time()\n",
        "        losses = []\n",
        "\n",
        "        for iter, (inputs, labels) in enumerate(train_loader):\n",
        "            \n",
        "            #BB - reset optimizer gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # both inputs and labels have to reside in the same device as the model's\n",
        "            inputs = inputs.type(torch.FloatTensor).to(device) #transfer the input to the same device as the model's\n",
        "            labels = labels.type(torch.LongTensor).to(device) #transfer the labels to the same device as the model's\n",
        "\n",
        "            outputs = fcn_model(inputs) #we will not need to transfer the output, it will be automatically in the same device as the model's!\n",
        "            \n",
        "            #BB\n",
        "            loss = criterion(outputs, labels) #calculate loss\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            #BB - backpropagate\n",
        "            loss.backward()\n",
        "\n",
        "            #BB - update the weights\n",
        "            optimizer.step()\n",
        "        \n",
        "        #Turn on if you want to mess with scheduling lr\n",
        "        #scheduler.step()\n",
        "            \n",
        "            if iter % 10 == 0:\n",
        "                print(\"epoch{}, iter{}, loss: {}\".format(epoch, iter, loss.item()))\n",
        "        \n",
        "        print(\"Finish epoch {}, time elapsed {}\".format(epoch, time.time() - ts))\n",
        "        \n",
        "\n",
        "        current_miou_score, current_acc, current_loss = val(epoch)\n",
        "        valid_loss.append(current_loss)\n",
        "        train_loss.append(np.mean(losses))\n",
        "        \n",
        "        \n",
        "        if current_miou_score > best_iou_score:\n",
        "            best_iou_score = current_miou_score\n",
        "            best_acc = current_acc\n",
        "            best_loss = current_loss\n",
        "\n",
        "            #Save to model\n",
        "            torch.save(fcn_model.state_dict(), FILE)\n",
        "\n",
        "        else:\n",
        "          epoch_no_imp += 1\n",
        "\n",
        "          if epoch_no_imp >= epoch_stop:\n",
        "            print(f'No improvement after epoch {epoch-epoch_stop}')\n",
        "            print(f'iou: {best_iou_score}, pixel acc: {best_acc}, loss: {best_loss}')\n",
        "            return train_loss, valid_loss\n",
        "            break\n",
        "    return train_loss, valid_loss\n",
        "            \n",
        "    \n",
        "\n",
        "def val(epoch):\n",
        "    fcn_model.eval() # Put in eval mode (disables batchnorm/dropout) !\n",
        "    \n",
        "    losses = []\n",
        "    mean_iou_scores = []\n",
        "    accuracy = []\n",
        "\n",
        "    with torch.no_grad(): # we don't need to calculate the gradient in the validation/testing\n",
        "\n",
        "        for iter, (input, label) in enumerate(val_loader):\n",
        "\n",
        "            # both inputs and labels have to reside in the same device as the model's\n",
        "            input = input.type(torch.FloatTensor).to(device) #transfer the input to the same device as the model's\n",
        "            label = label.type(torch.LongTensor).to(device) #transfer the labels to the same device as the model's\n",
        "\n",
        "            output = fcn_model(input)\n",
        "\n",
        "\n",
        "            loss = criterion(output, label) #calculate the loss\n",
        "            losses.append(loss.item()) #call .item() to get the value from a tensor. The tensor can reside in gpu but item() will still work \n",
        "\n",
        "            pred = torch.argmax(output, dim=1) # Make sure to include an argmax to get the prediction from the outputs of your model\n",
        "\n",
        "            mean_iou_scores.append(np.nanmean(iou(pred, label, n_class)))  # Complete this function in the util, notice the use of np.nanmean() here\n",
        "        \n",
        "            accuracy.append(pixel_acc(pred, label)) # Complete this function in the util\n",
        "\n",
        "\n",
        "    print(f\"\\nLoss at epoch: {epoch} is {np.mean(losses)}\")\n",
        "    print(f\"IoU at epoch: {epoch} is {np.mean(mean_iou_scores)}\")\n",
        "    print(f\"Pixel acc at epoch: {epoch} is {np.mean(accuracy)}\\n\")\n",
        "\n",
        "    fcn_model.train() #DONT FORGET TO TURN THE TRAIN MODE BACK ON TO ENABLE BATCHNORM/DROPOUT!!\n",
        "\n",
        "    return np.mean(mean_iou_scores), np.mean(accuracy), np.mean(losses)\n",
        "\n",
        "def test():\n",
        "\n",
        "    #Load model\n",
        "    fcn_model = FCN(n_class=n_class)\n",
        "    fcn_model.load_state_dict(torch.load(FILE))\n",
        "    fcn_model.to(device)\n",
        "\n",
        "    losses = []\n",
        "    mean_iou_scores = []\n",
        "    accuracy = []\n",
        "\n",
        "    with torch.no_grad(): # we don't need to calculate the gradient in the validation/testing\n",
        "\n",
        "        for iter, (input, label) in enumerate(test_loader):\n",
        "\n",
        "            # both inputs and labels have to reside in the same device as the model's\n",
        "            input = input.type(torch.FloatTensor).to(device) #transfer the input to the same device as the model's\n",
        "            label = label.type(torch.LongTensor).to(device)#transfer the labels to the same device as the model's\n",
        "\n",
        "            output = fcn_model(input)\n",
        "\n",
        "            loss = criterion(output, label) #calculate the loss\n",
        "            losses.append(loss.item()) \n",
        "\n",
        "            pred = torch.argmax(output, dim=1) \n",
        "            mean_iou_scores.append(np.nanmean(iou(pred, label, n_class)))\n",
        "            accuracy.append(pixel_acc(pred, label))\n",
        "\n",
        "    print(f\"Loss is {np.mean(losses)}\")\n",
        "    print(f\"IoU is {np.mean(mean_iou_scores)}\")\n",
        "    print(f\"Pixel acc is {np.mean(accuracy)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WdJ0I57d9Bcu"
      },
      "outputs": [],
      "source": [
        "    gc.collect() \n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.device_count() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7wVRdLq4z1q",
        "outputId": "ebaa88d9-6755-432e-9b75-54877cb0e2d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:126: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss at epoch: 0 is 2.1540205478668213\n",
            "IoU at epoch: 0 is 0.03069301570987771\n",
            "Pixel acc at epoch: 0 is 0.1844885632995873\n",
            "\n",
            "epoch0, iter0, loss: 2.3874359130859375\n",
            "epoch0, iter10, loss: 1.7612696886062622\n",
            "Finish epoch 0, time elapsed 59.130473613739014\n",
            "\n",
            "Loss at epoch: 0 is 22362.88671875\n",
            "IoU at epoch: 0 is 0.06216060216393744\n",
            "Pixel acc at epoch: 0 is 0.3145453685749465\n",
            "\n",
            "epoch1, iter0, loss: 1.5115562677383423\n",
            "epoch1, iter10, loss: 1.0521812438964844\n",
            "Finish epoch 1, time elapsed 57.94193911552429\n",
            "\n",
            "Loss at epoch: 1 is 6.869622707366943\n",
            "IoU at epoch: 1 is 0.0847652757605042\n",
            "Pixel acc at epoch: 1 is 0.32784163805267946\n",
            "\n",
            "epoch2, iter0, loss: 0.9782946109771729\n",
            "epoch2, iter10, loss: 1.006362795829773\n",
            "Finish epoch 2, time elapsed 57.543009996414185\n",
            "\n",
            "Loss at epoch: 2 is 1.9127963781356812\n",
            "IoU at epoch: 2 is 0.13930783686225964\n",
            "Pixel acc at epoch: 2 is 0.5566858056945647\n",
            "\n",
            "epoch3, iter0, loss: 0.845603883266449\n",
            "epoch3, iter10, loss: 0.7462591528892517\n",
            "Finish epoch 3, time elapsed 57.923908710479736\n",
            "\n",
            "Loss at epoch: 3 is 1.0268911123275757\n",
            "IoU at epoch: 3 is 0.24870022802781766\n",
            "Pixel acc at epoch: 3 is 0.6063000315421401\n",
            "\n",
            "epoch4, iter0, loss: 0.7717486619949341\n",
            "epoch4, iter10, loss: 0.6439974904060364\n",
            "Finish epoch 4, time elapsed 57.87638449668884\n",
            "\n",
            "Loss at epoch: 4 is 1.1191471219062805\n",
            "IoU at epoch: 4 is 0.19772932010100847\n",
            "Pixel acc at epoch: 4 is 0.6351215003551063\n",
            "\n",
            "epoch5, iter0, loss: 0.766322672367096\n",
            "epoch5, iter10, loss: 0.6021601557731628\n",
            "Finish epoch 5, time elapsed 57.85744380950928\n",
            "\n",
            "Loss at epoch: 5 is 0.7081137299537659\n",
            "IoU at epoch: 5 is 0.32079009419607835\n",
            "Pixel acc at epoch: 5 is 0.8222319754760063\n",
            "\n",
            "epoch6, iter0, loss: 0.8311224579811096\n",
            "epoch6, iter10, loss: 0.6189875602722168\n",
            "Finish epoch 6, time elapsed 57.795467376708984\n",
            "\n",
            "Loss at epoch: 6 is 0.9706213474273682\n",
            "IoU at epoch: 6 is 0.26426944048299644\n",
            "Pixel acc at epoch: 6 is 0.642391631800203\n",
            "\n",
            "epoch7, iter0, loss: 0.6689294576644897\n",
            "epoch7, iter10, loss: 0.5757853984832764\n",
            "Finish epoch 7, time elapsed 58.020315408706665\n",
            "\n",
            "Loss at epoch: 7 is 0.7043524384498596\n",
            "IoU at epoch: 7 is 0.33154926802380186\n",
            "Pixel acc at epoch: 7 is 0.7857774685841035\n",
            "\n",
            "epoch8, iter0, loss: 0.6504192352294922\n",
            "epoch8, iter10, loss: 0.6178131103515625\n",
            "Finish epoch 8, time elapsed 57.8962516784668\n",
            "\n",
            "Loss at epoch: 8 is 0.5504313707351685\n",
            "IoU at epoch: 8 is 0.38060362216955357\n",
            "Pixel acc at epoch: 8 is 0.8436848069443638\n",
            "\n",
            "epoch9, iter0, loss: 0.5282676219940186\n",
            "epoch9, iter10, loss: 0.6333892941474915\n",
            "Finish epoch 9, time elapsed 57.84949803352356\n",
            "\n",
            "Loss at epoch: 9 is 0.6228589117527008\n",
            "IoU at epoch: 9 is 0.3570438136351626\n",
            "Pixel acc at epoch: 9 is 0.7812251178122135\n",
            "\n",
            "epoch10, iter0, loss: 0.5151304602622986\n",
            "epoch10, iter10, loss: 0.5749086737632751\n",
            "Finish epoch 10, time elapsed 57.89313364028931\n",
            "\n",
            "Loss at epoch: 10 is 0.6438655853271484\n",
            "IoU at epoch: 10 is 0.3421269274814538\n",
            "Pixel acc at epoch: 10 is 0.7797589589267628\n",
            "\n",
            "epoch11, iter0, loss: 0.4467504918575287\n",
            "epoch11, iter10, loss: 0.43399253487586975\n",
            "Finish epoch 11, time elapsed 57.75354599952698\n",
            "\n",
            "Loss at epoch: 11 is 1.3650476932525635\n",
            "IoU at epoch: 11 is 0.19010468218595195\n",
            "Pixel acc at epoch: 11 is 0.5374381020414152\n",
            "\n",
            "epoch12, iter0, loss: 0.6024454832077026\n",
            "epoch12, iter10, loss: 0.6121699213981628\n",
            "Finish epoch 12, time elapsed 57.54692029953003\n",
            "\n",
            "Loss at epoch: 12 is 0.9843597412109375\n",
            "IoU at epoch: 12 is 0.2738796546852457\n",
            "Pixel acc at epoch: 12 is 0.6682229942070108\n",
            "\n",
            "epoch13, iter0, loss: 0.48820406198501587\n",
            "epoch13, iter10, loss: 0.4228067696094513\n",
            "Finish epoch 13, time elapsed 57.63309144973755\n",
            "\n",
            "Loss at epoch: 13 is 0.533696323633194\n",
            "IoU at epoch: 13 is 0.39355542975852864\n",
            "Pixel acc at epoch: 13 is 0.8198628265345207\n",
            "\n",
            "epoch14, iter0, loss: 0.45441359281539917\n",
            "epoch14, iter10, loss: 0.532791793346405\n",
            "Finish epoch 14, time elapsed 62.78433394432068\n",
            "\n",
            "Loss at epoch: 14 is 0.9111781716346741\n",
            "IoU at epoch: 14 is 0.31889587812728504\n",
            "Pixel acc at epoch: 14 is 0.6528140255793555\n",
            "\n",
            "epoch15, iter0, loss: 0.546690046787262\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    val(0)  # show the accuracy before training\n",
        "    train_loss, valid_loss = train()\n",
        "    test()\n",
        "    \n",
        "    # housekeeping\n",
        "    gc.collect() \n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbFpdAar8RoW"
      },
      "outputs": [],
      "source": [
        "train_loss\n",
        "valid_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMZtdXD7H956"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(train_loss,\"-b\", label=\"Training Loss\")\n",
        "plt.plot(valid_loss,\"-r\", label=\"Validation Loss\")\n",
        "plt.locator_params(axis=\"x\", integer=True, tight=True)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Transfer Learning (res34)\" + \" Training and Validation Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "# plt.savefig(title + ' Loss.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "plt.clf()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "transfer_learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
